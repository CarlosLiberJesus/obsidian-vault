# Introduction

**Status**: Open-source model by Microsoft. Phi-3 Mini (3.8B parameters, ~4GB VRAM at 4-bit) is already set up with Ollama, and you’re ready to test it.
**Local Deployment**: Possible—Phi-3 Mini fits your 6GB VRAM, and larger models like Phi-4 14B (noted in the second image) require 16GB VRAM.
**Priority**: High. Phi-3 Mini is your current testing model, and Phi-4 14B could be a future target alongside DeepSeek R1 14B.

## Known Versions

|            | Parameters | VRAM | Quantization | -   |
| ---------- | ---------- | ---- | ------------ | --- |
| Phi-3 Mini | 3.8B       | 4GB  | INT8         | -   |
| Phi-3 Mini | 3.8B       | 8GB  | FP16         | -   |
| Phi-4      | 14B        | 10GB | INT8         | -   |
| Phi-4      | 14B        | 16GB | FP16         | -   |
